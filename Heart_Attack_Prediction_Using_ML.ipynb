{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nfarhaan/Heart-Disease-Prediction-Using-Machine-Learning/blob/main/Heart_Attack_Prediction_Using_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYxYIXEDqb7M"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqHe4rxOxYQg",
        "outputId": "6d9a046f-11d3-4759-b2b7-c65eb54dfb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from kaggle) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from kaggle) (2.9.0)\n",
            "Requirement already satisfied: requests in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from kaggle) (2.2.2)\n",
            "Requirement already satisfied: bleach in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from requests->kaggle) (3.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\sarwi\\.conda\\envs\\word2vec\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMK4Z9hPxmX9",
        "outputId": "4cdc3232-d783-4774-e110-d0b491ad8f51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading heart-attack-analysis-prediction-dataset.zip to ./dataset/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0.00/4.11k [00:00<?, ?B/s]\n",
            "100%|██████████| 4.11k/4.11k [00:00<00:00, 2.08MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n",
            "License(s): unknown\n",
            "Downloading heart-disease-dataset.zip to ./dataset/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0.00/6.18k [00:00<?, ?B/s]\n",
            "100%|██████████| 6.18k/6.18k [00:00<00:00, 4.20MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/imnikhilanand/heart-attack-prediction\n",
            "License(s): CC0-1.0\n",
            "Downloading heart-attack-prediction.zip to ./dataset/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0.00/2.61k [00:00<?, ?B/s]\n",
            "100%|██████████| 2.61k/2.61k [00:00<00:00, 1.77MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/rishidamarla/heart-disease-prediction\n",
            "License(s): CC0-1.0\n",
            "Downloading heart-disease-prediction.zip to ./dataset/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0.00/3.41k [00:00<?, ?B/s]\n",
            "100%|██████████| 3.41k/3.41k [00:00<00:00, 3.49MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir dataset\\raw\\\n",
        "\n",
        "!kaggle datasets download -d rashikrahmanpritom/heart-attack-analysis-prediction-dataset -p ./dataset/raw\n",
        "!kaggle datasets download -d johnsmith88/heart-disease-dataset -p ./dataset/raw\n",
        "!kaggle datasets download -d imnikhilanand/heart-attack-prediction -p ./dataset/raw\n",
        "!kaggle datasets download -d rishidamarla/heart-disease-prediction -p ./dataset/raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "eNyF4L41LlpU"
      },
      "outputs": [],
      "source": [
        "# New Columns\n",
        "\n",
        "new_columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
        "                  'thalachh', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KhMFlx4Axy_l"
      },
      "outputs": [],
      "source": [
        "# Panda & NumPy Config\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmLK6l2dx-KH",
        "outputId": "e93f5656-47e8-48f9-ac7d-05da92031b2e"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/heart_attack_analysis.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the first dataset\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m dataset_1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/heart_attack_analysis.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset 1 Columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset_1\u001b[38;5;241m.\u001b[39mcolumns)\n",
            "File \u001b[1;32mc:\\Users\\sarwi\\.conda\\envs\\WORD2VEC\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\sarwi\\.conda\\envs\\WORD2VEC\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\sarwi\\.conda\\envs\\WORD2VEC\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\sarwi\\.conda\\envs\\WORD2VEC\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\sarwi\\.conda\\envs\\WORD2VEC\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/heart_attack_analysis.csv'"
          ]
        }
      ],
      "source": [
        "# Load the first dataset\n",
        "\n",
        "dataset_1 = pd.read_csv('/content/heart_attack_analysis.csv')\n",
        "print(\"Dataset 1 Columns:\")\n",
        "print(dataset_1.columns)\n",
        "\n",
        "# Keep only the required columns and rename them\n",
        "dataset_1 = dataset_1.iloc[:, :len(new_columns)]  # Keep only the first 14 columns\n",
        "dataset_1.columns = new_columns  # Rename columns\n",
        "print(dataset_1.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wzvVam2yCEJ",
        "outputId": "096eb715-00f7-4fbd-ce35-718e75a4e645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 2 Columns:\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load the second dataset\n",
        "\n",
        "dataset_2 = pd.read_csv('/content/heart_disease_dataset.csv')\n",
        "print(\"Dataset 2 Columns:\")\n",
        "print(dataset_2.columns)\n",
        "\n",
        "# Keep only the required columns and rename them\n",
        "dataset_2 = dataset_2.iloc[:, :len(new_columns)]  # Keep only the first 14 columns\n",
        "dataset_2.columns = new_columns  # Rename columns\n",
        "print(dataset_2.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RPF8dFZyEUc",
        "outputId": "18429974-4e85-4c47-cacb-67e5ff8df9c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 3 Columns:\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num       '],\n",
            "      dtype='object')\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load the third dataset\n",
        "\n",
        "dataset_3 = pd.read_csv('/content/heart_attack_prediction.csv')\n",
        "print(\"Dataset 3 Columns:\")\n",
        "print(dataset_3.columns)\n",
        "\n",
        "# Keep only the required columns and rename them\n",
        "dataset_3 = dataset_3.iloc[:, :len(new_columns)]  # Keep only the first 14 columns\n",
        "dataset_3.columns = new_columns  # Rename columns\n",
        "print(dataset_3.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sJvY6omyGUi",
        "outputId": "4e8b84dc-9672-4cbe-9ebf-52ddeef6398d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 4 Columns:\n",
            "Index(['Age', 'Sex', 'Chest pain type', 'BP', 'Cholesterol', 'FBS over 120',\n",
            "       'EKG results', 'Max HR', 'Exercise angina', 'ST depression',\n",
            "       'Slope of ST', 'Number of vessels fluro', 'Thallium', 'Heart Disease'],\n",
            "      dtype='object')\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load the fourth dataset\n",
        "\n",
        "dataset_4 = pd.read_csv('/content/heart_disease_prediction.csv')\n",
        "print(\"Dataset 4 Columns:\")\n",
        "print(dataset_4.columns)\n",
        "\n",
        "# Keep only the required columns and rename them\n",
        "dataset_4 = dataset_4.iloc[:, :len(new_columns)]  # Keep only the first 14 columns\n",
        "dataset_4.columns = new_columns  # Rename columns\n",
        "\n",
        "# Assuming the target column is the last column, convert 'Presence' to 1 and 'Absence' to 0\n",
        "dataset_4['target'] = dataset_4['target'].replace({'Presence': 1, 'Absence': 0})\n",
        "\n",
        "print(dataset_1.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhjKnhltOVEU",
        "outputId": "441c6c8f-14cd-477d-b358-e2ee961be632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged Dataset Columns:\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n",
            "Merged Dataset Preview:\n",
            "   age  sex  cp trestbps chol fbs restecg thalachh exang  oldpeak slope ca  \\\n",
            "0   63    1   3      145  233   1       0      150     0      2.3     0  0   \n",
            "1   37    1   2      130  250   0       1      187     0      3.5     0  0   \n",
            "2   41    0   1      130  204   0       0      172     0      1.4     2  0   \n",
            "3   56    1   1      120  236   0       1      178     0      0.8     2  0   \n",
            "4   57    0   0      120  354   0       1      163     1      0.6     2  0   \n",
            "\n",
            "  thal target  \n",
            "0    1      1  \n",
            "1    2      1  \n",
            "2    2      1  \n",
            "3    2      1  \n",
            "4    2      1  \n",
            "Number of Rows: 1892\n",
            "Number of Columns: 14\n"
          ]
        }
      ],
      "source": [
        "# Merge all datasets\n",
        "merged_dataset = pd.concat([dataset_1, dataset_2, dataset_3, dataset_4], ignore_index=True)\n",
        "\n",
        "# Display the merged dataset\n",
        "print(\"Merged Dataset Columns:\")\n",
        "print(merged_dataset.columns)\n",
        "\n",
        "# Display the first few rows of the merged dataset\n",
        "print(\"Merged Dataset Preview:\")\n",
        "print(merged_dataset.head())\n",
        "\n",
        "# Print the number of rows and columns\n",
        "rows, columns = merged_dataset.shape\n",
        "print(f\"Number of Rows: {rows}\")\n",
        "print(f\"Number of Columns: {columns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q5fgDVtYqVK",
        "outputId": "d49b613f-88ae-4cdd-b260-8526419d5a1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Rows with Missing Data (contains '?'): 293\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-d677b265e7da>:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  missing_data_rows = merged_dataset.applymap(lambda x: x == '?').any(axis=1)\n"
          ]
        }
      ],
      "source": [
        "# Identify rows with missing data represented by '?'\n",
        "missing_data_rows = merged_dataset.applymap(lambda x: x == '?').any(axis=1)\n",
        "\n",
        "# Count the number of rows with missing data\n",
        "num_missing_rows = missing_data_rows.sum()\n",
        "\n",
        "# Print the number of rows with missing data\n",
        "print(f\"Number of Rows with Missing Data (contains '?'): {num_missing_rows}\")\n",
        "\n",
        "# Replace '?' with NaN\n",
        "merged_dataset.replace('?', np.nan, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jssZtMX2U16r",
        "outputId": "cc70420f-276f-4244-ab64-6816a185a798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree Model Accuracy: 95.38%\n"
          ]
        }
      ],
      "source": [
        "# Decision Tree Classifier\n",
        "# Removed Empty Data Records\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Replace '?' with NaN\n",
        "merged_dataset.replace('?', np.nan, inplace=True)\n",
        "\n",
        "# Convert all columns to numeric\n",
        "merged_dataset = merged_dataset.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Drop rows where the target variable is NaN\n",
        "merged_dataset.dropna(subset=['target'], inplace=True)  # Replace 'target_column_name' with your actual target column name\n",
        "\n",
        "# Define features and target variable\n",
        "X = merged_dataset.drop('target', axis=1)  # Replace 'target_column_name' with your actual target column name\n",
        "y = merged_dataset['target']  # Replace 'target_column_name' with your actual target column name\n",
        "\n",
        "# Impute missing values in features using the mean of each column\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree classifier\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yforVW-ieUdu",
        "outputId": "9678451a-2cab-4506-a98c-6be99ad0fdc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neural Network Model Accuracy: 75.69%\n"
          ]
        }
      ],
      "source": [
        "# Neural Network Classifier\n",
        "# Removed Empty Data Records\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Replace '?' with NaN\n",
        "merged_dataset.replace('?', np.nan, inplace=True)\n",
        "\n",
        "# Convert all columns to numeric\n",
        "merged_dataset = merged_dataset.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Drop rows where the target variable is NaN\n",
        "merged_dataset.dropna(subset=['target'], inplace=True)  # Replace 'target' with your actual target column name\n",
        "\n",
        "# Define features and target variable\n",
        "X = merged_dataset.drop('target', axis=1)  # Replace 'target' with your actual target column name\n",
        "y = merged_dataset['target']  # Replace 'target' with your actual target column name\n",
        "\n",
        "# Impute missing values in features using the mean of each column\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Neural Network classifier\n",
        "model2 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Neural Network Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYGGvfgTe_OV",
        "outputId": "962a6073-1db4-483f-b589-5d983e6e8d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: No Heart Attack\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# prompt: predict using a record\n",
        "\n",
        "import numpy as np\n",
        "# Example new record (replace with your actual values)\n",
        "new_record = np.array([[67, 0, 3, 60, 200, 0, 0, 100, 0, 1.6, 0, 0, 7]])\n",
        "\n",
        "# Impute missing values if any (using the same imputer fitted earlier)\n",
        "new_record_imputed = imputer.transform(new_record)\n",
        "\n",
        "# Make prediction\n",
        "prediction = model2.predict(new_record_imputed)\n",
        "\n",
        "# Print prediction\n",
        "if prediction[0] == 0:\n",
        "    print(\"Prediction: No Heart Attack\")\n",
        "else:\n",
        "    print(\"Prediction: Heart Attack\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
