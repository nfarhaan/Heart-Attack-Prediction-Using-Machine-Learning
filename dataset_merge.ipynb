{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nfarhaan/Heart-Disease-Prediction-Using-Machine-Learning/blob/main/Heart_Attack_Prediction_Using_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYxYIXEDqb7M"
      },
      "source": [
        "## Curating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqHe4rxOxYQg",
        "outputId": "6d9a046f-11d3-4759-b2b7-c65eb54dfb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from kaggle) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: requests in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from kaggle) (2.2.2)\n",
            "Requirement already satisfied: bleach in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->kaggle) (3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tqdm->kaggle) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.1.1; however, version 24.2 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\farha\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMK4Z9hPxmX9",
        "outputId": "4cdc3232-d783-4774-e110-d0b491ad8f51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The syntax of the command is incorrect.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset\n",
            "License(s): CC0-1.0\n",
            "heart-attack-analysis-prediction-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Dataset URL: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n",
            "License(s): unknown\n",
            "heart-disease-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Dataset URL: https://www.kaggle.com/datasets/imnikhilanand/heart-attack-prediction\n",
            "License(s): CC0-1.0\n",
            "heart-attack-prediction.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Dataset URL: https://www.kaggle.com/datasets/rishidamarla/heart-disease-prediction\n",
            "License(s): CC0-1.0\n",
            "heart-disease-prediction.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Dataset URL: https://www.kaggle.com/datasets/abdmental01/heart-disease-dataset\n",
            "License(s): Community Data License Agreement - Sharing - Version 1.0\n",
            "Downloading heart-disease-dataset.zip to ./dataset/raw\n",
            "... resuming from 6325 bytes (20618 bytes left) ...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 23%|██▎       | 6.18k/26.3k [00:00<?, ?B/s]\n",
            "100%|██████████| 26.3k/26.3k [00:00<00:00, 496kB/s]\n"
          ]
        }
      ],
      "source": [
        "# Create the directory to store raw zipped datasets\n",
        "!mkdir -p dataset/raw/\n",
        "\n",
        "# Download the datasets from Kaggle\n",
        "!kaggle datasets download -d rashikrahmanpritom/heart-attack-analysis-prediction-dataset -p ./dataset/raw\n",
        "!kaggle datasets download -d johnsmith88/heart-disease-dataset -p ./dataset/raw\n",
        "!kaggle datasets download -d imnikhilanand/heart-attack-prediction -p ./dataset/raw\n",
        "!kaggle datasets download -d rishidamarla/heart-disease-prediction -p ./dataset/raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the dataset from figshare\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# Define the URL of the dataset\n",
        "url = \"https://figshare.com/ndownloader/files/36169122\"\n",
        "\n",
        "# Define the target directory and file name\n",
        "raw_data_dir = './dataset/raw/'\n",
        "csv_file_name = 'heart_dataset_figshare.csv'\n",
        "zip_file_name = 'heart_dataset_figshare.zip'\n",
        "csv_file_path = os.path.join(raw_data_dir, csv_file_name)\n",
        "zip_file_path = os.path.join(raw_data_dir, zip_file_name)\n",
        "\n",
        "# Ensure the target directory exists\n",
        "os.makedirs(raw_data_dir, exist_ok=True)\n",
        "\n",
        "# Step 2: Download the file\n",
        "response = requests.get(url)\n",
        "with open(csv_file_path, 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Step 3: Compress the downloaded file into a zip archive\n",
        "with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
        "    zipf.write(csv_file_path, os.path.basename(csv_file_path))\n",
        "\n",
        "# Optionally, delete the original CSV file after compression\n",
        "os.remove(csv_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "# Define paths\n",
        "raw_data_dir = 'dataset/raw'\n",
        "csv_data_dir = 'dataset/csv'\n",
        "\n",
        "# Ensure the csv_data_dir exists\n",
        "os.makedirs(csv_data_dir, exist_ok=True)\n",
        "\n",
        "# Unzip and rename for dataset 1, extracting only 'heart.csv'\n",
        "destination_file_1 = os.path.join(csv_data_dir, 'heart_dataset_1.csv')\n",
        "if not os.path.exists(destination_file_1):\n",
        "    with zipfile.ZipFile(os.path.join(raw_data_dir, 'heart-attack-analysis-prediction-dataset.zip'), 'r') as zip_ref:\n",
        "        if 'heart.csv' in zip_ref.namelist():\n",
        "            zip_ref.extract('heart.csv', csv_data_dir)\n",
        "            os.rename(os.path.join(csv_data_dir, 'heart.csv'), destination_file_1)\n",
        "\n",
        "# Unzip and rename for dataset 2\n",
        "destination_file_2 = os.path.join(csv_data_dir, 'heart_dataset_2.csv')\n",
        "if not os.path.exists(destination_file_2):\n",
        "    with zipfile.ZipFile(os.path.join(raw_data_dir, 'heart-disease-dataset.zip'), 'r') as zip_ref:\n",
        "        zip_ref.extractall(csv_data_dir)\n",
        "    os.rename(os.path.join(csv_data_dir, 'heart.csv'), destination_file_2)\n",
        "\n",
        "# Unzip and rename for dataset 3\n",
        "destination_file_3 = os.path.join(csv_data_dir, 'heart_dataset_3.csv')\n",
        "if not os.path.exists(destination_file_3):\n",
        "    with zipfile.ZipFile(os.path.join(raw_data_dir, 'heart-attack-prediction.zip'), 'r') as zip_ref:\n",
        "        zip_ref.extractall(csv_data_dir)\n",
        "    os.rename(os.path.join(csv_data_dir, 'data.csv'), destination_file_3)\n",
        "\n",
        "# Unzip and rename for dataset 4\n",
        "destination_file_4 = os.path.join(csv_data_dir, 'heart_dataset_4.csv')\n",
        "if not os.path.exists(destination_file_4):\n",
        "    with zipfile.ZipFile(os.path.join(raw_data_dir, 'heart-disease-prediction.zip'), 'r') as zip_ref:\n",
        "        zip_ref.extractall(csv_data_dir)\n",
        "    os.rename(os.path.join(csv_data_dir, 'Heart_Disease_Prediction.csv'), destination_file_4)\n",
        "\n",
        "# Unzip and rename for dataset 5\n",
        "destination_file_5 = os.path.join(csv_data_dir, 'heart_dataset_5.csv')  \n",
        "if not os.path.exists(destination_file_5):\n",
        "    with zipfile.ZipFile(os.path.join(raw_data_dir, 'heart_dataset_figshare.zip'), 'r') as zip_ref:\n",
        "        zip_ref.extractall(csv_data_dir)\n",
        "    os.rename(os.path.join(csv_data_dir, 'heart_dataset_figshare.csv'), destination_file_5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "eNyF4L41LlpU"
      },
      "outputs": [],
      "source": [
        "# New Columns\n",
        "\n",
        "new_columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
        "                  'thalachh', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KhMFlx4Axy_l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (1.24.4)\n",
            "Requirement already satisfied: pandas in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (2.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\farha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.1.1; however, version 24.2 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\farha\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "# Panda & NumPy Configuration\n",
        "%pip install numpy pandas\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmLK6l2dx-KH",
        "outputId": "e93f5656-47e8-48f9-ac7d-05da92031b2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 1 Columns:\n",
            "Index(['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
            "       'exng', 'oldpeak', 'slp', 'caa', 'thall', 'output'],\n",
            "      dtype='object')\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load the first dataset\n",
        "\n",
        "dataset_1 = pd.read_csv('./dataset/csv/heart_dataset_1.csv')\n",
        "print(\"Dataset 1 Columns:\")\n",
        "print(dataset_1.columns)\n",
        "\n",
        "# Keep only the required columns and rename them\n",
        "dataset_1 = dataset_1.iloc[:, :len(new_columns)]  # Keep only the first 14 columns\n",
        "dataset_1.columns = new_columns  # Rename columns\n",
        "print(dataset_1.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wzvVam2yCEJ",
        "outputId": "096eb715-00f7-4fbd-ce35-718e75a4e645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 2 Columns:\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load the second dataset\n",
        "\n",
        "dataset_2 = pd.read_csv('./dataset/csv/heart_dataset_2.csv')\n",
        "print(\"Dataset 2 Columns:\")\n",
        "print(dataset_2.columns)\n",
        "\n",
        "# Keep only the required columns and rename them\n",
        "dataset_2 = dataset_2.iloc[:, :len(new_columns)]  # Keep only the first 14 columns\n",
        "dataset_2.columns = new_columns  # Rename columns\n",
        "print(dataset_2.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RPF8dFZyEUc",
        "outputId": "18429974-4e85-4c47-cacb-67e5ff8df9c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 3 Columns:\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num       '],\n",
            "      dtype='object')\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load the third dataset\n",
        "\n",
        "dataset_3 = pd.read_csv('./dataset/csv/heart_dataset_3.csv')\n",
        "print(\"Dataset 3 Columns:\")\n",
        "print(dataset_3.columns)\n",
        "\n",
        "# Keep only the required columns and rename them\n",
        "dataset_3 = dataset_3.iloc[:, :len(new_columns)]  # Keep only the first 14 columns\n",
        "dataset_3.columns = new_columns  # Rename columns\n",
        "print(dataset_3.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sJvY6omyGUi",
        "outputId": "4e8b84dc-9672-4cbe-9ebf-52ddeef6398d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 4 Columns:\n",
            "Index(['Age', 'Sex', 'Chest pain type', 'BP', 'Cholesterol', 'FBS over 120',\n",
            "       'EKG results', 'Max HR', 'Exercise angina', 'ST depression',\n",
            "       'Slope of ST', 'Number of vessels fluro', 'Thallium', 'Heart Disease'],\n",
            "      dtype='object')\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load the fourth dataset\n",
        "\n",
        "dataset_4 = pd.read_csv('./dataset/csv/heart_dataset_4.csv')\n",
        "print(\"Dataset 4 Columns:\")\n",
        "print(dataset_4.columns)\n",
        "\n",
        "# Keep only the required columns and rename them\n",
        "dataset_4 = dataset_4.iloc[:, :len(new_columns)]  # Keep only the first 14 columns\n",
        "dataset_4.columns = new_columns  # Rename columns\n",
        "\n",
        "# Assuming the target column is the last column, convert 'Presence' to 1 and 'Absence' to 0\n",
        "dataset_4['target'] = dataset_4['target'].replace({'Presence': 1, 'Absence': 0})\n",
        "\n",
        "print(dataset_1.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 5 Columns:\n",
            "Index(['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
            "       'exng', 'oldpeak', 'slp', 'caa', 'thall', 'output'],\n",
            "      dtype='object')\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load the fifth dataset\n",
        "\n",
        "dataset_5 = pd.read_csv('./dataset/csv/heart_dataset_5.csv')\n",
        "print(\"Dataset 5 Columns:\")\n",
        "print(dataset_5.columns)\n",
        "\n",
        "# Keep only the required columns and rename them\n",
        "dataset_5 = dataset_5.iloc[:, :len(new_columns)]  # Keep only the first 14 columns\n",
        "dataset_5.columns = new_columns  # Rename columns\n",
        "print(dataset_5.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhjKnhltOVEU",
        "outputId": "441c6c8f-14cd-477d-b358-e2ee961be632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged Dataset Columns:\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n",
            "Number of Columns: 14\n",
            "Merged dataset saved to ./dataset/merged_csv/raw_merged_heart_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming dataset_1, dataset_2, dataset_3, and dataset_4 are already loaded and preprocessed\n",
        "\n",
        "# Merge all datasets\n",
        "merged_dataset = pd.concat([dataset_1, dataset_2, dataset_3, dataset_4, dataset_5], ignore_index=True)\n",
        "\n",
        "# Display the merged dataset columns\n",
        "print(\"Merged Dataset Columns:\")\n",
        "print(merged_dataset.columns)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('/dataset/merged_csv', exist_ok=True)\n",
        "print(f\"Number of Columns: {columns}\")\n",
        "\n",
        "# Save the merged dataset to a new CSV file\n",
        "output_file_path = './dataset/merged_csv/raw_merged_heart_dataset.csv'\n",
        "merged_dataset.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Merged dataset saved to {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q5fgDVtYqVK",
        "outputId": "d49b613f-88ae-4cdd-b260-8526419d5a1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Rows with Missing Data (contains '?' or NaN) removed: 293\n",
            "Number of Rows after dropping missing data: 1888\n",
            "Number of Columns: 14\n",
            "Cleaned dataset saved to ./dataset/merged_csv/cleaned_merged_heart_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Step 1: Load the merged dataset from the saved CSV file\n",
        "input_file_path = './dataset/merged_csv/raw_merged_heart_dataset.csv'\n",
        "merged_dataset = pd.read_csv(input_file_path)\n",
        "\n",
        "# Step 2: Drop rows with missing data represented by '?' or NaN\n",
        "# First, remove rows containing '?'\n",
        "cleaned_dataset = merged_dataset[~merged_dataset.isin(['?']).any(axis=1)]\n",
        "\n",
        "# Next, remove rows containing NaN values\n",
        "cleaned_dataset = cleaned_dataset.dropna()\n",
        "\n",
        "# Count the number of rows dropped\n",
        "num_rows_dropped = merged_dataset.shape[0] - cleaned_dataset.shape[0]\n",
        "print(f\"Number of Rows with Missing Data (contains '?' or NaN) removed: {num_rows_dropped}\")\n",
        "\n",
        "# Step 3: Save the cleaned dataset to a new CSV file\n",
        "cleaned_file_path = './dataset/merged_csv/cleaned_merged_heart_dataset.csv'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('./dataset/merged_csv', exist_ok=True)\n",
        "\n",
        "cleaned_dataset.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "# Display the number of rows and columns in the cleaned dataset\n",
        "print(f\"Number of Rows after dropping missing data: {cleaned_dataset.shape[0]}\")\n",
        "print(f\"Number of Columns: {cleaned_dataset.shape[1]}\")\n",
        "\n",
        "print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
